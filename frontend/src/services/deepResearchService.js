/**
 * FinBud Deep Research Chat Pipeline v1.0
 * 4-Step Process: Classifier ‚Üí Clarify&Validate ‚Üí Research Architect ‚Üí Display
 */

import { GoogleGenerativeAI } from "@google/generative-ai";
import axios from 'axios';
import { io } from "socket.io-client"; // For websocket logging
import pLimit from "p-limit";
import {jsonrepair } from 'jsonrepair';
// import Groq from "groq-sdk";
const GEMINI_API_KEY = process.env.VUE_APP_GEMINI_API_KEY;
const API_BASE_URL = process.env.VUE_APP_DEPLOY_URL; //"http://localhost:8888/.netlify/functions/server" 

if (!GEMINI_API_KEY || GEMINI_API_KEY === 'your_gemini_api_key_here') {
  console.warn('‚ö†Ô∏è GEMINI API KEY NOT CONFIGURED! Please set VUE_APP_GEMINI_API_KEY in your environment.');
}

// const GROQ_API_KEY = process.env.GROQ_API_KEY;
// if (!GROQ_API_KEY || GROQ_API_KEY === 'your_groq_api_key_here') {
//   console.warn('‚ö†Ô∏è GROQ API KEY NOT CONFIGURED! Please set VUE_APP_GROQ_API_KEY in your environment.');
// }

const geminiAI = GEMINI_API_KEY ? new GoogleGenerativeAI(GEMINI_API_KEY) : null;
// const groqClient = GROQ_API_KEY ? new Groq({ apiKey: GROQ_API_KEY }) : null;

const CHUNK_PAYLOAD_LIMIT     = 4500;       // soft cap per LLM request (tokens)
const SCRAPE_CONCURRENCY      = 4;          // simultaneous HTTP scrapes
const LEAF_LLM_CONCURRENCY    = 3;          // simultaneous summarize-leaf calls
const BRANCH_LLM_CONCURRENCY  = 2;          // simultaneous synthesize-branch calls
const DEBUG_SIZE_THRESHOLD    = 0.9;        // log when a request ‚â• 90 % of limit

// Simple token approximation function (avoids WebAssembly issues)
function approximateTokenCount(text) {
  if (!text) return 0;
  // Rough approximation: 1 token ‚âà 0.75 words for English text
  // This is less accurate than tiktoken but avoids browser compatibility issues
  const words = text.split(/\s+/).length;
  return Math.ceil(words * 1.33); // Convert words to approximate tokens
}

// interface NodeResult {
//   id: string;
//   name: string;
//   summary: string;            // synthesized text
//   deliverables: string[];     // rolled-up deliverable list
// }

export class DeepResearchService {
  constructor(logCallback) {
    this.currentStep = 1; // 1: Classifier, 2: Clarify, 3: Architect, 4...9: MECE Pipeline
    this.userPrompt = '';
    this.clarificationResponse = '';
    this.researchBrief = null;
    this.finalArchitecture = null;
    this.pipelineCompleted = false;
    this.pipelineStartIndex = 0;
    this.socket = null; // Websocket for logging
    this.logCallback = logCallback || console.log; // Callback to update UI with logs

    // per-instance tunables sourced from global constants
    this.CHUNK_PAYLOAD_LIMIT  = CHUNK_PAYLOAD_LIMIT;
    this.SCRAPE_CONCURRENCY   = SCRAPE_CONCURRENCY;
    this.LEAF_LLM_CONCURRENCY = LEAF_LLM_CONCURRENCY;
    this.BRANCH_LLM_CONCURRENCY = BRANCH_LLM_CONCURRENCY;

    // Centralized limiter for all backend LLM agent calls
    this.llmLimiter = pLimit(this.LEAF_LLM_CONCURRENCY);
  }

  // Main entry point - determines which step to executeAdd commentMore actions
  async processMessage(message, conversationHistory = []) {
    try {
      const totalUserMessages = conversationHistory.filter(msg => msg.role === 'user').length;
      
      // If pipeline was completed and this is a new user message, reset and start fresh
      if (this.pipelineCompleted) {
        console.log('üîÑ Pipeline was completed, starting new research cycle...');
        this.reset();
        this.pipelineStartIndex = totalUserMessages - 1; // Mark where new pipeline starts (current message will be +1)
        this.userPrompt = message;
        this.currentStep = 1;
        console.log(`üìç New pipeline starts at user message index: ${this.pipelineStartIndex}`);
        return await this.step1_GeminiClassifier(message);
      }
      
      // Calculate user messages count for current pipeline only
      const currentPipelineUserMessages = totalUserMessages - this.pipelineStartIndex;
      
      console.log(`üîÑ Deep Research Pipeline:`);
      console.log(`üìä Total user messages: ${totalUserMessages}`);
      console.log(`üìç Pipeline start index: ${this.pipelineStartIndex}`);
      console.log(`üìä Current pipeline user messages: ${currentPipelineUserMessages}`);
      console.log(`üìù Current message: "${message}"`);
      console.log(`üèóÔ∏è Current step: ${this.currentStep}`);
      console.log(`üèÅ Pipeline completed: ${this.pipelineCompleted}`);
      console.log(`üìú Conversation history:`, conversationHistory);
      
      if (currentPipelineUserMessages === 1) {
        // First user message - Step 1: Gemini Classifier
        this.userPrompt = message;
        this.currentStep = 1;
        return await this.step1_GeminiClassifier(message);
        
      } else if (currentPipelineUserMessages === 2) {
        // Second user message - Step 2 continuation or Step 3
        this.clarificationResponse = message;
        this.currentStep = 3; // Skip to step 3 (assume clarification complete)
        return await this.step3_ResearchArchitect();
        
      } else if (currentPipelineUserMessages === 0) {
        // Edge case: empty history
        this.userPrompt = message;
        this.currentStep = 1;
        return await this.step1_GeminiClassifier(message);
              } else {
          // Unexpected state
          console.warn(`‚ö†Ô∏è Unexpected conversation state. Current pipeline user messages: ${currentPipelineUserMessages}. Resetting...`);
          this.reset();
          this.pipelineStartIndex = totalUserMessages - 1; // Reset pipeline start (current message will be +1)
          this.userPrompt = message;
          this.currentStep = 1;
          return await this.step1_GeminiClassifier(message);
      }

    } catch (error) {
      console.error('Deep research pipeline error:', error);
      return `Xin l·ªói, c√≥ l·ªói trong Deep Research pipeline: ${error.message}. Vui l√≤ng th·ª≠ l·∫°i.`;
    }
  }

  // Step 1: Gemini Classifier API - Check if finance-related
  async step1_GeminiClassifier(userMessage) {
    try {
      console.log('üéØ STEP 1: Gemini Classifier API');
      
      if (!geminiAI) {
        console.warn('‚ö†Ô∏è Gemini API not available, skipping classification');
        // Skip to step 2 if no API
        this.currentStep = 2;
        return await this.step2_ClarifyAndValidate(userMessage);
      }
      
      const model = geminiAI.getGenerativeModel({ model: "gemini-2.0-flash" });
      
      const classifierPrompt = `# GEMINI CLASSIFIER API

Task: Classify if the following user input is related to Finance/Economics research.

Categories:
- "Finance/Economics": Questions about stocks, markets, investments, economic indicators, financial analysis, banking, cryptocurrency, risk management, monetary policy, etc.
- "Other": Everything else (cooking, sports, entertainment, general knowledge, etc.)

User Input: "${userMessage}"

Instructions:
- Respond with ONLY one word: "Finance/Economics" or "Other"
- No explanation needed

Classification:`;

      const result = await model.generateContent({
        contents: [{ role: "user", parts: [{ text: classifierPrompt }] }],
        generationConfig: {
          temperature: 0.1,
          maxOutputTokens: 50,
        },
      });

      const classification = result.response.text().trim();
      console.log('‚úÖ STEP 1 RESULT:', classification);
      
             if (classification.includes('Finance/Economics')) {
         // Proceed to Step 2
         this.currentStep = 2;
         console.log('‚úÖ STEP 1 ‚Üí STEP 2 (Finance/Economics confirmed)');
         return await this.step2_ClarifyAndValidate(userMessage);
       } else {
         // Out of scope - mark pipeline as completed to reset for next message
         console.log('‚úÖ STEP 1 ‚Üí OUT_OF_SCOPE');
         this.pipelineCompleted = true;
         return "Xin l·ªói, FinBud Deep Research ch·ªâ h·ªó tr·ª£ c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn t√†i ch√≠nh v√† kinh t·∫ø. Vui l√≤ng ƒë·∫∑t c√¢u h·ªèi v·ªÅ ph√¢n t√≠ch t√†i ch√≠nh, ƒë·∫ßu t∆∞, th·ªã tr∆∞·ªùng, ho·∫∑c ch√≠nh s√°ch kinh t·∫ø.";
       }

             } catch (error) {
       console.error('üö® Step 1 error:', error);
       // Fallback to step 2 if classification fails
       this.currentStep = 2;
       console.log('‚úÖ STEP 1 ‚Üí STEP 2 (Fallback due to error)');
       return await this.step2_ClarifyAndValidate(userMessage);
     }
  }

  // Step 2: Clarify & Validate Agent (Smart Ask) - Build 7-D metadata
  async step2_ClarifyAndValidate(userMessage) {
    try {
      console.log('üéØ STEP 2: Clarify & Validate Agent (Smart Ask)');
      
      if (!geminiAI) {
        // Fallback clarification
        return this.generateFallbackClarification(userMessage);
      }
      
      const model = geminiAI.getGenerativeModel({ model: "gemini-2.0-flash" });
      
      const clarifyPrompt = `# PROMPT A ‚Äî FinBud ¬∑ Clarify & Validate Agent (Smart Ask)

SYSTEM
You are FinBud's Clarify & Validate Agent.
Goal: From <USER_PROMPT>, extract full 7-D metadata by:
‚Ä¢ Auto-filling obvious fields (via NER/inference/default)
‚Ä¢ Asking max 1 turn (1‚Äì2 combo questions) to complete weak/conflicting fields

7-D Metadata Fields:
- domain: Asset Pricing & PM | Risk Management | Macro & Policy | Corporate Finance | FinTech & Digital Assets | Derivatives & Quant
- objective: analyze | forecast | compare | evaluate | optimize
- entities: specific companies, indices, sectors, or financial instruments
- time_horizon: historical periods, forecast periods (e.g., "2020-2024", "5-year analysis")
- geography: Global | Vietnam | US | Asia | Europe | etc.
- data_constraints: preferred data sources (Bloomberg, Reuters, SEC, etc.) or "no preference"
- output_preferences: report type and language ("detailed report, Vietnamese" or "executive summary, English")

USER_PROMPT: "${userMessage}"

Instructions:
1. Extract what you can automatically from the prompt
2. If you can infer most fields with confidence, return:
   READY_FOR_META
   {
     "domain": "...",
     "objective": "...", 
     "entities": "...",
     "time_horizon": "...",
     "geography": "...",
     "data_constraints": "...",
     "output_preferences": "..."
   }

3. If you need clarification on 1-2 key fields, ask ONLY 1 focused question (max 25 words):
   INCOMPLETE_BRIEF
   [Your clarification question here]

Analyze and respond:`;

      const result = await model.generateContent({
        contents: [{ role: "user", parts: [{ text: clarifyPrompt }] }],
        generationConfig: {
          temperature: 0.3,
          maxOutputTokens: 400,
        },
      });

      const response = result.response.text().trim();
      console.log('‚úÖ STEP 2 RESULT:', response);

             if (response.includes('READY_FOR_META')) {
         // Extract JSON and proceed to Step 3 directly
         const jsonMatch = response.match(/\{[\s\S]*\}/);
         if (jsonMatch) {
           try {
             this.researchBrief = JSON.parse(jsonrepair(jsonMatch[0]));
             this.currentStep = 3;
             console.log('‚úÖ STEP 2 ‚Üí Direct to STEP 3 (READY_FOR_META)');
             return await this.step3_ResearchArchitect();
           } catch (jsonError) {
             console.error('JSON parsing error:', jsonError);
           }
         }
       }
      
      if (response.includes('INCOMPLETE_BRIEF')) {
        // Return clarification question
        const question = response.replace('INCOMPLETE_BRIEF', '').trim();
        return question || this.generateFallbackClarification(userMessage);
      }

      // Default clarification if no clear signal
      return this.generateFallbackClarification(userMessage);

    } catch (error) {
      console.error('üö® Step 2 error:', error);
      return this.generateFallbackClarification(userMessage);
    }
  }

  // Step 3: Deep Research Architect - Generate 5-level MECE research roadmap
  async step3_ResearchArchitect() {
    try {
      console.log('üéØ STEP 3: Deep Research Architect (MECE Hierarchy Builder)');
      
      // Build research brief from available data
      if (!this.researchBrief) {
        this.researchBrief = await this.buildResearchBriefFromResponses();
      }

      console.log('üìä Research Brief for Step 3:', this.researchBrief);

      if (!geminiAI) {
        // Fallback JSON structure
        return this.generateFallbackArchitecture();
      }
      
      const model = geminiAI.getGenerativeModel({ model: "gemini-2.0-flash" });
      
//       const architectPrompt = `# META PROMPT ‚Äî FinBud ¬∑ Deep Research Architect (Advanced MECE Synthesis Engine)

// ## SYSTEM IDENTITY & CAPABILITIES
// You are FinBud's Elite Deep Research Architect AI, specialized in Meta Prompting for Complex Financial Reasoning. You operate with advanced structural decomposition and type-theoretic categorization capabilities.

// ## META PROMPTING FRAMEWORK APPLICATION

// ### 1. COMPLEX PROBLEM DECOMPOSITION
// Primary Challenge: Transform abstract financial research requirements into concrete, executable 5-level MECE architecture
// Sub-Problems:
// - Domain categorization and boundary definition
// - Strategic pillar identification with mutual exclusivity
// - Research topic hierarchical mapping
// - Implementation module specification
// - Execution step operationalization

// ### 2. PRELIMINARY FOUNDATIONAL CONTENT
// Research Brief Analysis:
// ${JSON.stringify(this.researchBrief, null, 2)}

// Theoretical Framework:
// - MECE Principle: Mutually Exclusive (no overlap), Collectively Exhaustive (complete coverage)
// - Type Theory Application: Each level represents a distinct categorical type with specific properties
// - Financial Domain Taxonomy: Asset Pricing & PM | Risk Management | Macro & Policy | Corporate Finance | FinTech & Digital Assets | Derivatives & Quant
// - Research Methodology Hierarchy: Strategic ‚Üí Tactical ‚Üí Operational ‚Üí Implementation ‚Üí Execution

// ### 3. STEP-BY-STEP STRUCTURED REASONING

// #### Meta-Question 1: Domain Boundary Definition
// **Reasoning Pattern**: Given research brief, identify primary and secondary domain intersections
// **Structural Template**: Domain(name, scope, boundary_conditions)
// **Expected Output**: Single root domain with clear definitional boundaries

// #### Meta-Question 2: Strategic Pillar Extraction  
// **Reasoning Pattern**: Decompose domain into 3-5 mutually exclusive strategic areas
// **Structural Template**: Pillar(strategic_focus, coverage_scope, sub_domains)
// **Expected Output**: Complete strategic coverage without overlap

// #### Meta-Question 3: Research Topic Hierarchical Mapping
// **Reasoning Pattern**: For each pillar, identify 2-4 research topics with logical progression
// **Structural Template**: Topic(research_focus, methodology_type, deliverable_category)
// **Expected Output**: Coherent research progression within each pillar

// #### Meta-Question 4: Implementation Module Specification
// **Reasoning Pattern**: Transform research topics into actionable modules with clear dependencies
// **Structural Template**: Module(implementation_focus, resource_requirements, success_criteria)
// **Expected Output**: Operational modules with defined inputs/outputs

// #### Meta-Question 5: Execution Step Operationalization
// **Reasoning Pattern**: Break down modules into discrete, executable steps with metadata
// **Structural Template**: Step(action, goal, data_sources, methods, deliverables)
// **Expected Output**: Granular execution plan with complete metadata

// ### 4. ADVANCED MECE SYNTHESIS RULES

// #### Structural Constraints:
// - Exactly 5 levels: Domain(0) ‚Üí Pillars(0.x) ‚Üí Topics(0.x.y) ‚Üí Modules(0.x.y.z) ‚Üí Steps(0.x.y.z.w)
// - ID Notation: Hierarchical dot notation (e.g., "0.2.1.3.1")
// - Node Structure: {name, id, children[]} for levels 1-4; {name, id, goal, data_sources, methods, deliverables} for level 5
// - Naming Convention: Concise, descriptive (‚â§5 words), domain-specific terminology

// #### Content Requirements:
// - Generate COMPREHENSIVE, DETAILED architecture with maximum depth and breadth
// - Each pillar should have 3-4 research topics (not just 1-2)
// - Each topic should have 3-4 implementation modules  
// - Each module should have 4-6 execution steps
// - Level 5 Steps MUST include: goal (specific, actionable), data_sources (detailed sources), methods (specific techniques), deliverables (concrete outputs)
// - Financial domain relevance for entity: ${this.researchBrief.entities}
// - Geographic scope: ${this.researchBrief.geography}
// - Temporal focus: ${this.researchBrief.time_horizon}
// - Research objective: ${this.researchBrief.objective}

// #### Quality Assurance:
// - Mutual Exclusivity: No content overlap between sibling nodes
// - Collective Exhaustiveness: Complete coverage of parent scope
// - Logical Progression: Each level builds upon previous level
// - Implementation Feasibility: All steps must be executable with available tools/data

// ## OUTPUT SPECIFICATION
// CRITICAL: Generate COMPREHENSIVE, COMPLETE, VALID JSON ONLY. Maximum depth and detail required.

// Required Structure:
// - Exactly 5 levels deep with MAXIMUM breadth at each level
// - Domain ‚Üí 3-4 Strategic Pillars ‚Üí 3-4 Topics each ‚Üí 3-4 Modules each ‚Üí 4-6 Steps each
// - Every node must have: name, id, children (except level 5)
// - Level 5 nodes must have: name, id, goal, data_sources, methods, deliverables
// - Keep names concise but descriptive (‚â§5 words)
// - Ensure proper JSON syntax with balanced braces
// - GOAL: Create the most comprehensive research architecture possible

// Example minimal structure:
// {
//   "root": {
//     "name": "Domain",
//     "id": "0",
//     "children": [
//       {
//         "name": "Pillar One",
//         "id": "0.1",
//         "children": [
//           {
//             "name": "Topic Alpha", 
//             "id": "0.1.1",
//             "children": [
//               {
//                 "name": "Module A",
//                 "id": "0.1.1.1", 
//                 "children": [
//                   {
//                     "name": "Step 1",
//                     "id": "0.1.1.1.1",
//                     "goal": "objective",
//                     "data_sources": ["source1"],
//                     "methods": ["method1"],
//                     "deliverables": ["output1"]
//                   }
//                 ]
//               }
//             ]
//           }
//         ]
//       }
//     ]
//   }
// }

// Generate complete, valid JSON architecture now:`;




const architectPrompt = `
# FinBud ‚Äî Simplified Research Architect

## TASK
Generate a MECE financial research architecture (max 3 levels) for:

${JSON.stringify(this.researchBrief, null, 2)}

## STRUCTURE

- Output valid nested JSON
- Max 3 levels:
  - Level 0: Domain
  - Level 1: Topics
  - Level 2: Execution Blocks (leaf nodes)

## NODE FORMATS

- Root:
{
  "root": {
    "name": "Main Domain Name",
    "id": "0",
    "children": [ ... ]
  }
}

- Topic Nodes (Level 1):
{
  "name": "...",
  "id": "...",
  "children": [ ... ]
}

- Execution Blocks (Level 2 / leaves):
{
  "name": "...",
  "id": "...",
  "goal": "...",
  "data_sources": ["..."],
  "methods": ["..."],
  "deliverables": ["..."]
}

## RULES

- Mutually Exclusive Topics
- Collectively Exhaustive full coverage
- 3-5 Topics at Level 1
- 2-4 Execution Blocks per topic
- Use domain-specific finance terminology.
- Fully aligned with:
  - Domain: ${this.researchBrief.domain}
  - Entity: ${this.researchBrief.entities}
  - Objective: ${this.researchBrief.objective}
  - Geography: ${this.researchBrief.geography}
  - Time Horizon: ${this.researchBrief.time_horizon}

## OUTPUT

Return ONLY valid JSON inside code block:

\`\`\`json
{ full architecture here }
\`\`\`

Do not include any explanation. Do not generate anything else.
Begin.
`;



      const result = await model.generateContent({
        contents: [{ role: "user", parts: [{ text: architectPrompt }] }],
        generationConfig: {
          temperature: 0.3,
          maxOutputTokens: 8000, // Further increased for deep, detailed architectures
        },
      });

      const response = result.response.text().trim();
      console.log('‚úÖ STEP 3 RESULT (first 500 chars):', response.substring(0, 500) + '...');
      console.log('üìä Full response length:', response.length, 'characters');

      // Enhanced JSON cleaning and validation
      let cleanJson = response
        .replace(/```json\n?/g, '')
        .replace(/```\n?/g, '')
        .replace(/^[^{]*{/, '{') // Remove anything before first {
        .replace(/}[^}]*$/, '}') // Remove anything after last }
        .trim();
      
      console.log('üßπ Cleaned JSON length:', cleanJson.length, 'characters');
      console.log('üîç JSON preview (last 200 chars):', cleanJson.slice(-200));
      
      try {
        // Validate JSON structure before parsing
        if (!cleanJson.startsWith('{') || !cleanJson.endsWith('}')) {
          throw new Error('Invalid JSON structure - missing braces');
        }
        
        this.finalArchitecture = JSON.parse(jsonrepair(cleanJson));
        
        // Validate architecture structure
        if (!this.finalArchitecture.root || !this.finalArchitecture.root.children) {
          throw new Error('Invalid architecture structure - missing root or children');
        }
        
        this.currentStep = 4;
        
        // Add processing delay for deep research effect
        console.log('üî¨ Initiating Deep Research Processing...');
        console.log('‚ö° Meta Prompting Framework synthesizing...');
        await new Promise(resolve => setTimeout(resolve, 2000)); // 2 second thinking effect
        console.log('‚úÖ Deep Research Architecture synthesis complete!');
        
          // ** NEW: Kick off the MECE pipeline instead of displaying result directly **
          return await this.runMECEPipeline();
      } catch (jsonError) {
        console.error('JSON parsing error in Step 3:', jsonError);
        console.error('‚ùå Failed JSON content:', cleanJson.substring(0, 1000) + '...');
        console.log('üîÑ Falling back to simplified architecture...');
        return this.generateFallbackArchitecture();
      }

    } catch (error) {
      console.error('üö® Step 3 error:', error);
      return this.generateFallbackArchitecture();
    }
  }

  // =================================================================
  // == NEW MECE PIPELINE WORKFLOW (Steps from user diagram)
  // =================================================================

  /**
   * Main orchestrator for the MECE-based research workflow (new implementation).
   * Keeps the overall control-flow identical, but relies on the new size-aware
   * helpers and map-reduce summarisation pipeline.
   */
  async runMECEPipeline() {
    this._connectLogger();
    this._logProgress('üöÄ **Starting Deep Research Pipeline**');
    this._logProgress('Step 1: Parsing MECE Research Plan... Done.');

    try {
     
      // Step 2-5 ‚Äî DFS traversal, leaf processing and branch synthesis
      this._logProgress(
        'Step 2-5: Began Task Orchestration (DFS traversal, leaf processing, and branch synthesis)...'
      );
      const rootSummary = await this._sequentialProcess();
      if (!rootSummary) throw new Error('Failed to process research tree ‚Äì no root summary');

      // Step 6 ‚Äî Master compilation
      this._logProgress('‚úÖ Branch synthesis complete.');
      this._logProgress('Step 6: Compiling Master Report...');
      const masterReport = await this._compileMasterReport(rootSummary);
      if (!masterReport) throw new Error('Failed to compile master report');
      this._logProgress('‚úÖ Master report compiled.');

      // Step 7 ‚Äî QA / Fact-checking
      // this._logProgress('Step 7: Performing QA and Fact-Checking...');
      // const { finalReport, analytics } = await this._runQAChecks(masterReport);
      // if (!finalReport) throw new Error('QA checks failed');
      // this._logProgress(`‚úÖ QA Complete. Efficiency Analytics: ${JSON.stringify(analytics)}`);

      // Step 8-9 ‚Äî Output + user review
      this._logProgress('Step 8 & 9: Generating final output and awaiting user review...');
      const finalResult = this._prepareFinalResponse(masterReport);
      this._logProgress('‚úÖ **Research Complete!**');

      // Generate follow-up questions for user
      let followUpQuestions = [];
      try {
        followUpQuestions = await this._generateFollowUpQuestions(finalResult);
      } catch (e) {
        console.warn('‚ö†Ô∏è Failed to generate follow-up questions:', e.message);
      }
      
      return {
        report: finalResult,
        ticker: this.researchBrief.ticker,
        relevantQuestions: followUpQuestions,
      };
    } catch (err) {
      this._logProgress(`üí• **Pipeline Failed**: ${err.message}`);
      throw err;
    } finally {
      if (this.socket) this.socket.disconnect();
    }
  }

 

  _flattenMECE(node, list = []) {
    if (node.goal) {
      list.push({
        id: node.id,
        name: node.name,
        goal: node.goal,
        data_sources: node.data_sources || [],
      });
    }
  
    if (node.children && node.children.length > 0) {
      for (const child of node.children) {
        this._flattenMECE(child, list);
      }
    }
    return list;
  }

  async _sequentialProcess() {
    const taskList = this._flattenMECE(this.finalArchitecture.root);
    const taskSummaries = [];
  
    for (const task of taskList) {
      try {
        this._logProgress(`Processing Task: "${task.name}"...`);
        const rawData = await this._gatherData(task.data_sources);
        if (!rawData?.length) {
          this._logProgress(`‚ö†Ô∏è No data gathered for "${task.name}". Skipping.`);
          continue;
        }
  
        // üîß New inline source-attached content preparation
        const contents = rawData.map(r => `SOURCE: ${r.source}\n\nCONTENT:\n${r.content}`);
        
        const summary = await this._safeSummarize(contents, task.goal);
        taskSummaries.push({ id: task.id, name: task.name, summary });
  
        this._logProgress(`‚úÖ Task "${task.name}" summarized.`);
      } catch (err) {
        this._logProgress(`‚ö†Ô∏è Task failed "${task.name}": ${err.message}`);
      }
    }
  
    const fusedSynthesis = await this._mergeTaskSummaries(taskSummaries);
    return fusedSynthesis;
  }
  
  async _mergeTaskSummaries(taskSummaries) {
    const fullText = taskSummaries
      .map((t) => `### ${t.name}\n${t.summary}`)
      .join("\n\n");
  
    const { data } = await this.llmLimiter(() =>
      axios.post(`${API_BASE_URL}/agents/compile-report`, {
        synthesis: fullText,
        researchBrief: this.researchBrief,
      })
    );
  
    return data?.report || fullText;
  }
  

  // ======================================================================
  // ==  Real-time logging helpers  (unchanged)
  // ======================================================================
  _connectLogger() {
    this.socket = io(`${API_BASE_URL}/deep-research`);
    this.socket.on('connect', () =>
      console.log('üîå WebSocket connected to deep research namespace for real-time logging.')
    );
    this.socket.on('disconnect', () =>
      console.log('üîå WebSocket disconnected from deep research namespace.')
    );
  }

  _logProgress(message) {
    console.log(`[LOG] ${message}`);
    this.logCallback?.(message);
    if (this.socket?.connected) {
      this.socket.emit('progress', { message });
    }
  }

  // ======================================================================
  // ==  Size helpers  ‚Äì token counting & chunk slicing
  // ======================================================================
  _countTokens(text = '') {
    // Use simple word-based approximation instead of tiktoken to avoid WebAssembly issues
    return approximateTokenCount(text);
  }

  _sliceByTokens(text = '') {
    const limit = this.CHUNK_PAYLOAD_LIMIT || 3000;
    // For simplicity, use character-based chunking with approximation
    const avgCharsPerToken = 4; // Rough approximation
    const charLimit = limit * avgCharsPerToken;
    
    const slices = [];
    for (let i = 0; i < text.length; i += charLimit) {
      slices.push(text.slice(i, i + charLimit));
    }
    return slices.length > 0 ? slices : [text];
  }

  // ======================================================================
  // ==  Recursive map-reduce summarisation helpers
  // ======================================================================
  async _summarizeLongDoc(doc, goal, sources) {
    const slices = this._sliceByTokens(doc);
    const sliceSummaries = await Promise.all(
      slices.map((s) => this._summarizeChunk([s], goal, sources))
    );
    return this._summarizeChunk(sliceSummaries, goal, sources);
  }

  async _summarizeChunk(chunkArray, goal) {
    try {
      const { data } = await this.llmLimiter(() => axios.post(`${API_BASE_URL}/agents/summarize-leaf`, {
        contents: chunkArray,
        goal,
      }));
      if (!data?.summary) throw new Error('summarize-leaf returned no summary');
      return data.summary;
    } catch (err) {
      this._logProgress(`‚ö†Ô∏è summarize-leaf backend error (${goal}): ${err.message}. Falling back to verbatim.`);
      return this._truncate(chunkArray.join('\n'), 3000);
    }
  }
  

  /**
   * Entry-point wrapper that copes with arbitrary-sized documents:
   *  ‚Ä¢ oversize docs ‚Üí recursive map-reduce
   *  ‚Ä¢ small docs    ‚Üí single call
   *  ‚Ä¢ then packs everything into ‚â§ CHUNK_PAYLOAD_LIMIT buckets and summarises again
   */
  async _safeSummarize(rawDocs = [], goal = '', sources=[]) {
    const limit = this.CHUNK_PAYLOAD_LIMIT || 3000;
    const smallDocs = [];
    const longDocTasks = [];

    // 1Ô∏è‚É£ Split docs by size
    for (const doc of rawDocs) {
      if (this._countTokens(doc) > limit) {
        longDocTasks.push(this._summarizeLongDoc(doc, goal, sources));
      } else {
        smallDocs.push(doc);
      }
    }

    // 2Ô∏è‚É£ Await large-doc reductions
    const longDocSummaries = await Promise.all(longDocTasks);
    const allDocs = [...smallDocs, ...longDocSummaries];

    // 3Ô∏è‚É£ Pack docs into token-bounded buckets
    const buckets = [];
    let bucket = [];
    let bucketTokens = 0;

    for (const doc of allDocs) {
      const tok = this._countTokens(doc);
      if (bucketTokens + tok > limit) {
        buckets.push(bucket);
        bucket = [];
        bucketTokens = 0;
      }
      bucket.push(doc);
      bucketTokens += tok;
    }
    if (bucket.length) buckets.push(bucket);

    // 4Ô∏è‚É£ Map: summarise each bucket (with fallback captured inside _summarizeChunk)
    const bucketSummaries = [];
    for (const b of buckets) {
      try {
        bucketSummaries.push(await this._summarizeChunk(b, goal, sources));
      } catch (err) {
        this._logProgress(`\t\t- ‚ö†Ô∏è Bucket summarization failed: ${err.message}. Skipping bucket.`);
      }
    }

    // 5Ô∏è‚É£ Reduce: final summary (handle possible failure again)
    try {
      return await this._summarizeChunk(bucketSummaries, goal, sources);
    } catch (e) {
      // if even reduced summary fails, concatenate bucket summaries
      this._logProgress(`\t\t- ‚ö†Ô∏è Final summarization failed for ${goal}. Using concatenated bucket summaries.`);
      return this._truncate(bucketSummaries.join('\n'), 3000);
    }
  }

 
  // ======================================================================
  // ==  Data gathering (search + scrape with bounded concurrency)
  // ======================================================================
  async _gatherData(searchKeywords = [], options = {}) {
    const { numLinks = 3 } = options;
    if (!searchKeywords.length) throw new Error('No search keywords provided');

    const query = searchKeywords.join(' ');
    this._logProgress(`\t\t- Searching: "${query}" (max ${numLinks} links)`);
    const { data: searchData } = await axios.post(`${API_BASE_URL}/agents/search`, {
      query,
      numLinks,
    });

    const urls = searchData?.links ?? [];
    if (!urls.length) throw new Error('Search returned 0 URLs');

    const concurrency = this.SCRAPE_CONCURRENCY || 3;
    this._logProgress(`\t\t- Scraping ${urls.length} URLs (‚â§${concurrency} concurrently)...`);

    const limit = typeof pLimit === 'function' ? pLimit(concurrency) : (fn) => fn();
    const scrapePromises = urls.map((url) =>
      limit(async () => {
        try {
          const { data } = await axios.post(`${API_BASE_URL}/agents/scrape`, { url });
          this._logProgress(`\t\t- ‚úÖ Scraped: ${url}`);
          return{ source: data.source, content: data.content };
        } catch (err) {
          this._logProgress(`\t\t- ‚ö†Ô∏è Failed ${url}: ${err.message}`);
          return null;
        }
      })
    );

    const results = await Promise.all(scrapePromises);
    const successful = results.filter((c) => !!c);
    if (!successful.length) throw new Error('All scrapes failed');
    return successful;
  }

  // ======================================================================
  // ==  Branch synthesis, report compilation, QA, and final response
  // ==  (unchanged ‚Äì kept from earlier implementation)
  // ======================================================================
  async _compileMasterReport(rootSynthesis) {
    const MAX_CHARS_PER_COMPILE = 12000; // keep well below model limit

    const chunkBySize = (text, limit) => {
      const paras = text.split(/\n\s*\n/);
      const chunks = [];
      let current = '';
      for (const p of paras) {
        if ((current + p).length > limit) {
          if (current.trim()) chunks.push(current.trim());
          current = p.length > limit ? '' : p + '\n\n';
          if (p.length > limit) {
            for (let i = 0; i < p.length; i += limit) {
              chunks.push(p.slice(i, i + limit));
            }
          }
        } else {
          current += p + '\n\n';
        }
      }
      if (current.trim()) chunks.push(current.trim());
      return chunks;
    };

    this._logProgress(`\t- Step 7 (Master Compiler & Styling)...`);

    if (rootSynthesis.length < MAX_CHARS_PER_COMPILE) {
      const { data } = await this.llmLimiter(() => axios.post(`${API_BASE_URL}/agents/compile-report`, {
        synthesis: rootSynthesis,
        researchBrief: this.researchBrief,
      }));
      return data.report;
    }

    const chunks = chunkBySize(rootSynthesis, MAX_CHARS_PER_COMPILE);
    this._logProgress(`\t- Master synthesis too large ‚Äì splitting into ${chunks.length} chunks for compilation.`);

    const partialReports = [];
    for (let i = 0; i < chunks.length; i++) {
      const { data } = await this.llmLimiter(() => axios.post(`${API_BASE_URL}/agents/compile-report`, {
        synthesis: chunks[i],
        researchBrief: this.researchBrief,
      }));
      if (data?.report) partialReports.push(`## Section ${i + 1}\n\n${data.report}`);
    }

    const fusedReport = partialReports.join('\n\n');
    return fusedReport;
  }

  async _runQAChecks(report) {
    this._logProgress(`\t- Step 8 (QA & Fact-Check Loop)...`);
    const response = await this.llmLimiter(() => axios.post(`${API_BASE_URL}/agents/qa-fact-check`, { report }));
    return response.data;
  }

  _prepareFinalResponse(finalReport) {
    this.pipelineCompleted = true;
    const briefText = `**Research Brief:**
- **Topic:** ${this.researchBrief?.entities || this.userPrompt}
- **Objective:** ${this.researchBrief?.objective || 'analysis'}
- **Scope:** ${this.researchBrief?.geography || 'Global'} | ${this.researchBrief?.time_horizon || 'not specified'}
---`;
    return `${briefText}\n\n${finalReport}\n\n---\n*Feedback: Was this report helpful? (üëç/üëé)*`;
  }

  // Step 4: Display result JSON to UI
  step4_DisplayResult() {
    console.log('üéØ STEP 4: Display Result');
    
    // Mark pipeline as completed
    this.pipelineCompleted = true;
    console.log('‚úÖ Pipeline marked as completed - ready for new research cycle');
    
    const formattedJson = JSON.stringify(this.finalArchitecture, null, 2);
    
    return `üéØ **Deep Research Architecture Complete**

**üìã Research Brief:**
- **Domain:** ${this.researchBrief?.domain || 'T√†i ch√≠nh'}
- **Entity:** ${this.researchBrief?.entities || this.userPrompt}
- **Geography:** ${this.researchBrief?.geography || 'To√†n c·∫ßu'}
- **Objective:** ${this.researchBrief?.objective || 'analyze'}
- **Time Horizon:** ${this.researchBrief?.time_horizon || 'Ph√¢n t√≠ch l·ªãch s·ª≠'}

**üèóÔ∏è MECE Research Architecture:**
\`\`\`json
${formattedJson}
\`\`\``;
  }

  // Helper methods
  generateFallbackClarification(userPrompt) {
    const isVietnamese = /[√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]/i.test(userPrompt);
    
    if (isVietnamese) {
      return "B·∫°n mu·ªën ph√¢n t√≠ch trong kho·∫£ng th·ªùi gian n√†o? (v√≠ d·ª•: 2020-2024, 5 nƒÉm g·∫ßn ƒë√¢y)";
    } else {
      return "What time period would you like to analyze? (e.g., 2020-2024, last 5 years)";
    }
  }

  async buildResearchBriefFromResponses() {
    const prompt = `
  You are FinBud's Financial Research Extractor Agent.
  
  ## TASK
  
  Extract the structured research brief from the following user query:
  
  USER QUERY:
  ${this.userPrompt}
  
  ## OUTPUT FORMAT
  
  Return a valid JSON object as:
  
  {
    "domain": "...",             
    "objective": "...",
    "ticker": "...",
    "entities": "...",           
    "time_horizon": "...",       
    "geography": "...",          
    "data_constraints": "no preference",
    "output_preferences": "detailed report, Vietnamese"
  }
  
  ## EXTRACTION RULES
  
  - Always fill all fields, even if approximate.
  - Use best judgment based on query.
  - Output valid JSON only, inside code block: \`\`\`json { ... } \`\`\`
  
  BEGIN:
  `;
  
    try {
      // Use Gemini directly instead of calling the backend relay
      if (!geminiAI) {
        throw new Error('Gemini AI client not initialized');
      }

      const model = geminiAI.getGenerativeModel({ model: "gemini-2.0-flash" });

      // Respect existing rate-limiting wrapper
      const rawResult = await this.llmLimiter(() =>
        model.generateContent({
          contents: [{ role: "user", parts: [{ text: prompt }] }],
          generationConfig: {
            temperature: 0.3,
            maxOutputTokens: 400,
          },
        })
      );

      // Conform to previous `{ response: ... }` shape expected downstream
      const data = {
        response: rawResult.response.text().trim(),
      };
  
      const extractedJson = JSON.parse(jsonrepair(data?.response));
      return extractedJson;
    } catch (err) {
      this._logProgress(`üí• Failed extracting research brief: ${err.message}`);
      throw err;
    }
  }
  

  generateFallbackArchitecture() {
    this.finalArchitecture = {
      "root": {
        "name": "Asset Pricing & PM",
        "id": "0",
        "children": [
          {
            "name": "Market Analysis",
            "id": "0.0",
            "children": [
              {
                "name": "Price Discovery",
                "id": "0.0.0",
                "children": [
                  {
                    "name": "Data Collection Module",
                    "id": "0.0.0.0",
                    "children": [
                      {
                        "name": "Collect Market Data",
                        "id": "0.0.0.0.0",
                        "goal": "Gather relevant financial data for analysis",
                        "data_sources": ["Yahoo Finance", "Bloomberg", "Local exchanges"],
                        "methods": ["API calls", "Web scraping", "Database queries"],
                        "deliverables": ["Raw dataset", "Data quality report"]
                      }
                    ]
                  }
                ]
              }
            ]
          }
        ]
      }
    };
    
    return this.step4_DisplayResult();
  }

  _truncate(str = '', max = 3000) {
    return str.length > max ? str.slice(0, max) + '‚Ä¶(truncated)' : str;
  }

  // ---- REPLACED helper: hierarchical synthesis with size-aware chunking ----
  async _hierarchicalSynthesize(goal, childrenSummaries) {
    const MAX_PROMPT_CHARS = 12000; // safety margin
    const MAX_CHILD_SUMMARY_CHARS = 3500;

    // Pre-truncate any oversize child summary
    childrenSummaries = childrenSummaries.flatMap((child) => {
      if ((child.summary?.length || 0) <= MAX_CHILD_SUMMARY_CHARS) return [child];
      // slice summary into chunks and create pseudo children
      const parts = [];
      for (let i = 0; i < child.summary.length; i += MAX_CHILD_SUMMARY_CHARS) {
        parts.push({
          name: `${child.name} ‚Äì part ${(i / MAX_CHILD_SUMMARY_CHARS) + 1}`,
          summary: child.summary.slice(i, i + MAX_CHILD_SUMMARY_CHARS) + '‚Ä¶',
        });
      }
      return parts;
    });

    // Batch children to fit within prompt limits
    const batches = [];
    let currentBatch = [];
    let currentCost = 0;

    for (const child of childrenSummaries) {
      const childCost = (child.name?.length || 0) + (child.summary?.length || 0);
      if (currentCost + childCost > MAX_PROMPT_CHARS && currentBatch.length > 0) {
        batches.push(currentBatch);
        currentBatch = [child];
        currentCost = childCost;
      } else {
        currentBatch.push(child);
        currentCost += childCost;
      }
    }
    if (currentBatch.length > 0) batches.push(currentBatch);

    // Synthesize each batch
    const batchSyntheses = [];
    for (const batch of batches) {
      try {
        const { data } = await this.llmLimiter(() => axios.post(`${API_BASE_URL}/agents/synthesize-branch`, {
          goal,
          childrenSummaries: batch,
        }));
        if (data?.synthesis) batchSyntheses.push(data.synthesis);
      } catch (err) {
        this._logProgress(`\t\t- ‚ö†Ô∏è Branch synthesis failed for batch: ${err.message}`);
        // Fallback: concatenate child summaries
        const fallback = batch.map(c => `**${c.name}**: ${c.summary}`).join('\n\n');
        batchSyntheses.push(this._truncate(fallback, 3000));
      }
    }

    // If multiple batches, synthesize the batch results
    if (batchSyntheses.length > 1) {
      try {
        const pseudoChildren = batchSyntheses.map((synthesis, i) => ({
          name: `${goal} - Batch ${i + 1}`,
          summary: synthesis,
        }));
        const { data } = await this.llmLimiter(() => axios.post(`${API_BASE_URL}/agents/synthesize-branch`, {
          goal,
          childrenSummaries: pseudoChildren,
        }));
        return data?.synthesis || batchSyntheses.join('\n\n');
      } catch (err) {
        this._logProgress(`\t\t- ‚ö†Ô∏è Final batch synthesis failed: ${err.message}`);
        return this._truncate(batchSyntheses.join('\n\n'), 6000);
      }
    }

    return batchSyntheses[0] || '';
  }

  async _generateFollowUpQuestions(report) {
    if (!geminiAI) return [];
    try {
      const model = geminiAI.getGenerativeModel({ model: "gemini-1.5-flash" });
      const prompt = `You are FinBud. Read the following research report and suggest 3 concise follow-up research questions a finance analyst might ask next in Vietnamese. Return ONLY a JSON array of strings.\n\nREPORT:\n${report}`;
      const result = await model.generateContent({
        contents: [{ role: "user", parts: [{ text: prompt }] }],
        generationConfig: { temperature: 0.5, maxOutputTokens: 200 },
      });
      const text = result.response.text().trim();
      const cleaned = text.replace(/```json|```/g, '').trim();
      const arr = JSON.parse(jsonrepair(cleaned));
      return Array.isArray(arr) ? arr.slice(0, 5) : [];
    } catch (err) {
      console.error('Follow-up question generation failed:', err);
      return [];
    }
  }

  reset() {
    this.currentStep = 1;
    this.userPrompt = '';
    this.clarificationResponse = '';
    this.researchBrief = null;
    this.finalArchitecture = null;
    this.pipelineCompleted = false;
    if (this.socket) {
        this.socket.disconnect();
    }
    // Note: pipelineStartIndex is NOT reset here - it's set when starting new pipeline
    console.log('üîÑ Deep Research Service reset - ready for new pipeline');
  }
}

// Export singleton instance
export const deepResearchService = new DeepResearchService();
