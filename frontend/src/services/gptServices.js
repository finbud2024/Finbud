import axios from "axios";
import { OpenAI } from "openai";
import { GoogleGenerativeAI } from "@google/generative-ai";

// Environment variables
const OPENAI_API_KEY = process.env.VUE_APP_OPENAI_API_KEY;
const GEMINI_API_KEY = process.env.VUE_APP_GEMINI_API_KEY;
const DEEPSEEK_API_KEY = process.env.VUE_APP_DEEPSEEK_API_KEY;

// Initialize Gemini
const geminiAI = new GoogleGenerativeAI(GEMINI_API_KEY);

// Provider order for fallback strategy
const PROVIDER_FALLBACK_ORDER = ["openai", "gemini", "deepseek"];

/**
 * Attempts to call a function with different providers until successful or all providers fail
 * @param {Function} actionFn Function to call with different providers
 * @param {Array} providersToTry Array of provider names to try
 * @param {Object} options Additional options to pass to the function
 */
async function tryWithFallback(actionFn, providersToTry, options = {}) {
  // Create a copy of the provider array to avoid modifying the original
  const providers = [...providersToTry];
  let lastError = null;

  while (providers.length > 0) {
    const currentProvider = providers.shift();
    try {
      console.log(`ü§ñ TRYING API PROVIDER: ${currentProvider.toUpperCase()} ü§ñ`);
      const result = await actionFn(currentProvider, options);
      console.log(`‚úÖ SUCCESSFULLY USED ${currentProvider.toUpperCase()} API ‚úÖ`);
      return result;
    } catch (err) {
      lastError = err;
      console.warn(`‚ùå ERROR WITH ${currentProvider.toUpperCase()} API: ${err.message}`);
      
      // Check if this is a rate limit error (429) or token exhaustion
      const isRateLimit = err.response?.status === 429 || err.response?.status === 401; // <-- th√™m 401 ·ªü ƒë√¢y
      const isTokenExhaustion = 
        err.message.includes("rate limit") || 
        err.message.includes("quota exceeded") || 
        err.message.includes("token") || 
        err.message.includes("limit") ||
        err.response?.status === 401; 
        
      if (isRateLimit || isTokenExhaustion) {
        if (providers.length > 0) {
          console.log(`‚ö†Ô∏è RATE LIMIT OR TOKEN EXHAUSTION DETECTED. SWITCHING TO NEXT PROVIDER... ‚ö†Ô∏è`);
          // Continue to next provider
        } else {
          // All providers exhausted
          console.error(`‚ùå‚ùå ALL PROVIDERS EXHAUSTED. LAST ERROR: ${err.message} ‚ùå‚ùå`);
          throw new Error(`All providers exhausted. Last error: ${err.message}`);
        }
      } else {
        // For non-rate limit errors, don't try other providers
        console.error(`üõë NON-RATE LIMIT ERROR WITH ${currentProvider.toUpperCase()}: ${err.message} üõë`);
        throw err;
      }
    }
  }
  
  // If we get here, all providers failed
  throw lastError || new Error("All providers failed with unknown errors");
}

export const GPTService = {
  async generateStructuredJson(options) {
    const {
      messages,
      json_schema,
      provider = "auto", // "auto", "openai", "gemini", or "deepseek"
      model,
      temperature = 0.7,
      maxTokens = 1500,
    } = options;

    console.log(`üìä GENERATING STRUCTURED JSON (Provider: ${provider === "auto" ? "AUTO - will try all" : provider.toUpperCase()}) üìä`);

    // If set to auto, try all providers in fallback order
    if (provider === "auto") {
      return await tryWithFallback(
        async (currentProvider) => {
          return await this._generateStructuredJsonWithProvider({
            provider: currentProvider,
            messages,
            json_schema,
            model,
            temperature,
            maxTokens,
          });
        },
        PROVIDER_FALLBACK_ORDER
      );
    } else {
      // Use specified provider
      console.log(`üîç DIRECTLY USING ${provider.toUpperCase()} FOR JSON GENERATION üîç`);
      return await this._generateStructuredJsonWithProvider({
        provider,
        messages,
        json_schema,
        model,
        temperature,
        maxTokens,
      });
    }
  },

  async _generateStructuredJsonWithProvider(options) {
    const {
      provider,
      messages,
      json_schema,
      model,
      temperature,
      maxTokens,
    } = options;

    // Get default model based on provider
    const defaultModel = {
      openai: "gpt-3.5-turbo",
      gemini: "gemini-2.0-flash",
      deepseek: "deepseek/deepseek-chat:free"
    }[provider];
    
    const selectedModel = model || defaultModel;
    console.log(`ü§ñ USING ${provider.toUpperCase()} WITH MODEL: ${selectedModel} ü§ñ`);

    if (provider === "openai") {
      try {
        console.log(`‚è≥ SENDING REQUEST TO OPENAI API...`);
        const response = await axios.post(
          "https://api.openai.com/v1/chat/completions",
          {
            model: selectedModel,
            messages: messages,
            temperature: temperature,
            max_tokens: maxTokens,
            response_format: {
              type: "json_object",
              json_schema,
            },
          },
          {
            headers: {
              Authorization: `Bearer ${OPENAI_API_KEY}`,
              "Content-Type": "application/json",
            },
          }
        );
        console.log(`‚úÖ OPENAI RESPONSE RECEIVED SUCCESSFULLY`);

        const jsonString =
          response.data.choices[0]?.message?.content.trim() || "{}";

        try {
          const jsonData = JSON.parse(jsonString);
          return jsonData;
        } catch (parseError) {
          console.error("Error parsing JSON response from OpenAI:", parseError);
          return {}; // Return empty object on parse error
        }
      } catch (err) {
        console.error(`‚ùå ERROR GENERATING STRUCTURED JSON WITH OPENAI: ${err.message}`);
        throw err;
      }
    } else if (provider === "gemini") {
      try {
        console.log(`‚è≥ SENDING REQUEST TO GEMINI API...`);
        // For Gemini, convert chat messages to a format it can understand
        const geminiModel = geminiAI.getGenerativeModel({ model: selectedModel });
        
        // Extract system prompt and user messages
        const systemPrompt = messages.find(msg => msg.role === "system")?.content || "";
        const userMessages = messages
          .filter(msg => msg.role === "user")
          .map(msg => msg.content)
          .join("\n\n");
        
        // Format prompt with schema information
        const formattedPrompt = `
${systemPrompt}

User input: ${userMessages}

Please generate a JSON response that strictly follows this schema:
${JSON.stringify(json_schema, null, 2)}
        `;
        
        const result = await geminiModel.generateContent({
          contents: [{ role: "user", parts: [{ text: formattedPrompt }] }],
          generationConfig: { 
            temperature: temperature,
            maxOutputTokens: maxTokens,
          },
        });
        console.log(`‚úÖ GEMINI RESPONSE RECEIVED SUCCESSFULLY`);
        
        const jsonString = result.response.text().trim() || "{}";
        
        try {
          const jsonData = JSON.parse(jsonString);
          return jsonData;
        } catch (parseError) {
          console.error("Error parsing JSON response from Gemini:", parseError);
          return {}; // Return empty object on parse error
        }
      } catch (err) {
        console.error(`‚ùå ERROR GENERATING STRUCTURED JSON WITH GEMINI: ${err.message}`);
        throw err;
      }
    } else if (provider === "deepseek") {
      try {
        console.log(`‚è≥ SENDING REQUEST TO DEEPSEEK API...`);
        const response = await axios.post(
          "https://api.deepseek.com/v1/chat/completions",
          {
            model: selectedModel,
            messages: messages,
            temperature: temperature,
            max_tokens: maxTokens,
            response_format: {
              type: "json_object",
              json_schema,
            },
          },
          {
            headers: {
              Authorization: `Bearer ${DEEPSEEK_API_KEY}`,
              "Content-Type": "application/json",
            },
          }
        );
        console.log(`‚úÖ DEEPSEEK RESPONSE RECEIVED SUCCESSFULLY`);

        const jsonString =
          response.data.choices[0]?.message?.content.trim() || "{}";

        try {
          const jsonData = JSON.parse(jsonString);
          return jsonData;
        } catch (parseError) {
          console.error("Error parsing JSON response from DeepSeek:", parseError);
          return {}; // Return empty object on parse error
        }
      } catch (err) {
        console.error(`‚ùå ERROR GENERATING STRUCTURED JSON WITH DEEPSEEK: ${err.message}`);
        throw err;
      }
    } else {
      throw new Error(`Unsupported AI provider: ${provider}`);
    }
  }
};

export async function gptServices(payload, provider = "auto") {
  // Default system message for FinBud
  const defaultSystemMessage = {
    role: "system",
    content: `B·∫°n l√† FinBud ‚Äî m·ªôt tr·ª£ l√Ω t√†i ch√≠nh th√¥ng minh, th√¢n thi·ªán, chuy√™n n√≥i chuy·ªán b·∫±ng ti·∫øng Vi·ªát.
    Tuy nhi√™n, n·∫øu ng∆∞·ªùi d√πng d√πng ng√¥n ng·ªØ kh√°c, b·∫°n c√≥ th·ªÉ ph·∫£n h·ªìi b·∫±ng ng√¥n ng·ªØ ƒë√≥ cho ph√π h·ª£p.
    H√£y lu√¥n tr·∫£ l·ªùi m·ªôt c√°ch vui v·∫ª, d·ªÖ hi·ªÉu, nh∆∞ m·ªôt ng∆∞·ªùi b·∫°n ƒë√°ng tin c·∫≠y c·ªßa Gen Z. üòé
    N·∫øu tin nh·∫Øn ng∆∞·ªùi d√πng kh√¥ng r√µ r√†ng, h√£y l·ªãch s·ª± nh·∫Øc h·ªç vi·∫øt l·∫°i r√µ h∆°n, v√† ph·∫£n h·ªìi b·∫±ng **ti·∫øng Vi·ªát**.`,
  };

  // Combine default system message with payload from client
  const fullMessages = [defaultSystemMessage, ...payload];

  console.log(`üí¨ STARTING CHAT SERVICE (Provider: ${provider === "auto" ? "AUTO - will try all" : provider.toUpperCase()}) üí¨`);

  // If auto mode, try all providers with fallback
  if (provider === "auto") {
    return await tryWithFallback(
      (currentProvider) => _gptServicesWithProvider(fullMessages, currentProvider),
      PROVIDER_FALLBACK_ORDER
    );
  } else {
    // Use specified provider
    console.log(`üîç DIRECTLY USING ${provider.toUpperCase()} FOR CHAT üîç`);
    return await _gptServicesWithProvider(fullMessages, provider);
  }
}

async function _gptServicesWithProvider(messages, provider) {
  console.log(`ü§ñ PROCESSING CHAT WITH ${provider.toUpperCase()} ü§ñ`);
  
  if (provider === "openai") {
    try {
      console.log(`‚è≥ SENDING CHAT REQUEST TO OPENAI API...`);
      const response = await axios.post(
        "https://api.openai.com/v1/chat/completions",
        {
          model: "gpt-3.5-turbo",
          messages: messages,
          temperature: 0.7,
          max_tokens: 1000,
        },
        {
          headers: {
            Authorization: `Bearer ${OPENAI_API_KEY}`,
            "Content-Type": "application/json",
          },
        }
      );
      console.log(`‚úÖ OPENAI CHAT RESPONSE RECEIVED SUCCESSFULLY`);
      const answer = response.data.choices[0]?.message?.content.trim() || "";
      return answer;
    } catch (err) {
      console.error(`‚ùå ERROR IN CHAT WITH OPENAI: ${err.message}`);
      throw err;
    }
  } else if (provider === "gemini") {
    try {
      console.log(`‚è≥ SENDING CHAT REQUEST TO GEMINI API...`);
      const geminiModel = geminiAI.getGenerativeModel({ model: "gemini-2.0-flash" });
      
      // Extract system message
      const systemPrompt = messages.find(msg => msg.role === "system")?.content || "";
      
      // Convert user messages
      const userMessages = messages
        .filter(msg => msg.role === "user")
        .map(msg => msg.content)
        .join("\n\n");
      
      // Format prompt with system instructions first
      const formattedPrompt = `${systemPrompt}\n\nUser input: ${userMessages}`;
      
      const result = await geminiModel.generateContent({
        contents: [{ role: "user", parts: [{ text: formattedPrompt }] }],
        generationConfig: { 
          temperature: 0.7,
          maxOutputTokens: 1000,
        },
      });
      console.log(`‚úÖ GEMINI CHAT RESPONSE RECEIVED SUCCESSFULLY`);
      
      const answer = result.response.text().trim() || "";
      return answer;
    } catch (err) {
      console.error(`‚ùå ERROR IN CHAT WITH GEMINI: ${err.message}`);
      throw err;
    }
  } else if (provider === "deepseek") {
    try {
      console.log(`‚è≥ SENDING CHAT REQUEST TO DEEPSEEK API...`);
      const response = await axios.post(
        "https://api.deepseek.com/v1/chat/completions",
        {
          model: "deepseek-chat",
          messages: messages,
          temperature: 0.7,
          max_tokens: 1000,
        },
        {
          headers: {
            Authorization: `Bearer ${DEEPSEEK_API_KEY}`,
            "Content-Type": "application/json",
          },
        }
      );
      console.log(`‚úÖ DEEPSEEK CHAT RESPONSE RECEIVED SUCCESSFULLY`);
      const answer = response.data.choices[0]?.message?.content.trim() || "";
      return answer;
    } catch (err) {
      console.error(`‚ùå ERROR IN CHAT WITH DEEPSEEK: ${err.message}`);
      throw err;
    }
  } else {
    throw new Error(`Unsupported AI provider: ${provider}`);
  }
}

export async function gptNewsService(payload, trendingEvents, provider = "auto") {
  const defaultSystemMessage = {
    role: "system",
    content: `
  Your name is FinBud, and focus exclusively on relevant financial info and sorting headlines of a list of article headlines.
  -------- TASK --------
    1. Read all article headlines from the user. 
    2. Display the top 3 articles based on these priorities:
      - Microeconomics of America
      - Effects on the U.S. stock market
      - Trending scandals
    3. You should read these 3 articles to stay updated on the latest financial trends
    4. Always remember your name is FinBud, and focus exclusively on relevant financial info and sorting headlines.
  --------  FORMAT --------
  - **ONLY** Respond in a list of article titles and the reasons to read them
  - **ONLY** Return the top 3 in the exact format (with no extra text):
    <div style="font-size: 1.2rem;">You need to read these 3 articles:</div>
    1. <a href="ARTICLE_URL" target="_blank">"ARTICLE_TITLE"</a>: <span class="reason">"REASON_TO_READ_THEM"</span><br>
    2. <a href="ARTICLE_URL" target="_blank">"ARTICLE_TITLE"</a>: <span class="reason">"REASON_TO_READ_THEM"</span><br>
    3. <a href="ARTICLE_URL" target="_blank">"ARTICLE_TITLE"</a>: <span class="reason">"REASON_TO_READ_THEM"</span><br>
  -------- SAMPLES --------
  1. <a href="URL" target="_blank">"Stock Market Updates: Sensex, Nifty Flat At Open; Bank, Auto Stocks Gain"</a>: <span class="reason">This article offers stock market updates, helping you grasp short-term investment trends.</span><br>
  2. <a href="URL" target="_blank">"Indian stock market opens higher, Sensex above 74,600"</a>: <span class="reason">It provides a detailed analysis of India's market movements and their impact on global indices.</span><br>
  `,
  };

  const eventsList = trendingEvents
    .map(
      (event, index) =>
        `${index + 1}. <a href="${event.url}" target="_blank">${event.title}</a>`
    )
    .join("<br>"); // Use <br> to separate articles

  const eventsUserMessage = {
    role: "user",
    content: `Here are the trending article headlines:\n\n${eventsList}`,
  };

  const fullMessages = [defaultSystemMessage, ...payload, eventsUserMessage];

  console.log(`üì∞ STARTING NEWS SERVICE (Provider: ${provider === "auto" ? "AUTO - will try all" : provider.toUpperCase()}) üì∞`);
  console.log(`üìã PROCESSING ${trendingEvents.length} TRENDING EVENTS`);

  // If auto mode, try all providers with fallback
  if (provider === "auto") {
    return await tryWithFallback(
      (currentProvider) => _gptNewsServiceWithProvider(fullMessages, currentProvider),
      PROVIDER_FALLBACK_ORDER
    );
  } else {
    // Use specified provider
    console.log(`üîç DIRECTLY USING ${provider.toUpperCase()} FOR NEWS SERVICE üîç`);
    return await _gptNewsServiceWithProvider(fullMessages, provider);
  }
}

async function _gptNewsServiceWithProvider(messages, provider) {
  console.log(`ü§ñ PROCESSING NEWS WITH ${provider.toUpperCase()} ü§ñ`);
  
  if (provider === "openai") {
    try {
      console.log(`‚è≥ SENDING NEWS REQUEST TO OPENAI API...`);
      const response = await axios.post(
        "https://api.openai.com/v1/chat/completions",
        {
          model: "gpt-3.5-turbo",
          messages: messages,
          temperature: 0.7,
          max_tokens: 1000,
        },
        {
          headers: {
            Authorization: `Bearer ${OPENAI_API_KEY}`,
            "Content-Type": "application/json",
          },
        }
      );
      console.log(`‚úÖ OPENAI NEWS RESPONSE RECEIVED SUCCESSFULLY`);
      const answer = response.data.choices[0]?.message?.content.trim() || "";
      return answer;
    } catch (err) {
      console.error(`‚ùå ERROR IN NEWS SERVICE WITH OPENAI: ${err.message}`);
      throw err;
    }
  } else if (provider === "gemini") {
    try {
      console.log(`‚è≥ SENDING NEWS REQUEST TO GEMINI API...`);
      const geminiModel = geminiAI.getGenerativeModel({ model: "gemini-2.0-flash" });
      
      // For news service, we'll format the prompt differently to maintain the strict output format
      const systemInstructions = messages.find(msg => msg.role === "system")?.content || "";
      const userMessages = messages
        .filter(msg => msg.role === "user")
        .map(msg => msg.content)
        .join("\n\n");
      
      // Create a single prompt with all the necessary information
      const formattedPrompt = `
${systemInstructions}

${userMessages}
      `;
      
      const result = await geminiModel.generateContent({
        contents: [{ role: "user", parts: [{ text: formattedPrompt }] }],
        generationConfig: { 
          temperature: 0.7,
          maxOutputTokens: 1000,
        },
      });
      console.log(`‚úÖ GEMINI NEWS RESPONSE RECEIVED SUCCESSFULLY`);
      
      const answer = result.response.text().trim() || "";
      return answer;
    } catch (err) {
      console.error(`‚ùå ERROR IN NEWS SERVICE WITH GEMINI: ${err.message}`);
      throw err;
    }
  } else if (provider === "deepseek") {
    try {
      console.log(`‚è≥ SENDING NEWS REQUEST TO DEEPSEEK API...`);
      const response = await axios.post(
        "https://api.deepseek.com/v1/chat/completions",
        {
          model: "deepseek-chat",
          messages: messages,
          temperature: 0.7,
          max_tokens: 1000,
        },
        {
          headers: {
            Authorization: `Bearer ${DEEPSEEK_API_KEY}`,
            "Content-Type": "application/json",
          },
        }
      );
      console.log(`‚úÖ DEEPSEEK NEWS RESPONSE RECEIVED SUCCESSFULLY`);
      const answer = response.data.choices[0]?.message?.content.trim() || "";
      return answer;
    } catch (err) {
      console.error(`‚ùå ERROR IN NEWS SERVICE WITH DEEPSEEK: ${err.message}`);
      throw err;
    }
  } else {
    throw new Error(`Unsupported AI provider: ${provider}`);
  }
}